{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#tinyspark","title":"TinySpark","text":"<p>Welcome to TinySpark, the online learning platform that teaches you how to create smart machine learning applications on tiny devices. </p> <p>Tiny Machine Learning (TinyML) is an emerging field of technology that combines deep learning and embedded systems to enable artificial intelligence on resource-constrained hardware. It has many applications in various domains, such as agriculture, environmental sensing, speech recognition, healthcare, security and more.</p> <p>On the TinySpark platform, you will interactively learn the basics of neural networks, their structure, mathematics and how to train them. The theoretical learning is alternated with mini-projects that deepen your understanding, leading to fun, engaging and interesting insights into TinyML.</p> <p>To follow along with the projects and examples, you will need a TinySpark development board, which is a tiny device that packs enough processing power and sensors to start your TinyML journey.</p> <p>Get started!</p> <p></p> <p>Last updated: </p>"},{"location":"templates/","title":"MkDocs Material template page","text":"<p>Templates and Snippets for MKDocs</p>"},{"location":"templates/#heading-1","title":"Heading 1","text":""},{"location":"templates/#heading-2","title":"Heading 2","text":""},{"location":"templates/#heading-3","title":"Heading 3","text":"<p>This is some example text, just some words to see what's up with this documentation system, even some sp&amp;cia/ ch\u00e5r\u00e4ct\u00e9rs @nd 0th3r \u00f8d\u00eeti\u20acs!</p> <p>The document can also contain some bold text, or some italic text or some superscript or some subscript or some highlighted text, we can also add some keyboard keys like Ctrl+Alt+Del.</p> <p>More information on text formatting can be found in the Formatting reference.</p> <p>Additionally, block-quotes can be inserted:</p> <p>This is a blockquote containing some nonsence</p> <p>The blockquote can also contain multiple lines</p> <p>Lists can also be generated, there are two types, Ordered Lists and Unordered lists</p> <ol> <li>This is the first list item</li> <li>Another item on the ordered list<ol> <li>Nested ordered list</li> <li>another nested item</li> </ol> </li> <li>Third item in the list<ul> <li>unordered nested list</li> <li>inside an ordered list</li> </ul> </li> </ol> <p>We need to separate lists either by text or double newlines.</p> <ul> <li>This is the first unordered item</li> <li>The second item<ul> <li>Nested unordered lists</li> <li>Another nested item</li> </ul> </li> <li>The third unordered item<ol> <li>Ordered lists inside of unordered lists</li> <li>And another ordered item</li> </ol> </li> </ul> <p>Next, we might want to add some code to the document. Adding some <code>inline code</code> is pretty easy. Alternatively, we can also make a typeless code-block:</p> <pre><code>  function codeblock(String content) {\n    return content.html;\n  }\n\n  String text = \"This is some test content\";\n  codeblock(text);\n</code></pre> <p>If we want our codeblock to be styled according to a certain coding language, with line highlighting and annotations:</p> <pre><code>text = \"This is some test content\"\ndef codeBlock(content):\nhtml = content.html  # (1)!\nreturn html\ncodeBlock(text)\n</code></pre> <ol> <li>This is the annotation that will get added to the codeblock above</li> </ol> <p>Or another coding language, with title and annotations:</p> codeBlock.js<pre><code>let text = \"This is some test content\"\nfunction code_block(content) =&gt; {\nreturn content.html\n}\ncode_block(text)\n</code></pre> <p>A 'Open in Colab' badge can be added as follows:</p> <p></p> <p>More information can be found in the Code reference.</p> <p>We can divide some content using a horizontal rule:</p> <p>(External) content can be linked to websites or other pages of the website.</p> <p>Images and  get added in a similar way.</p> <p>Placeholder images can be added like this:</p> <p></p> <p>If a citation is needed, footnotes1 and other citations can be added.</p> <p>Call-outs can include side stories and other less important or extra content. More info can be found in the Admonitions reference.</p> <p>Note</p> <p>This is a admonitium or call-out with the note style, this should be displayed as such.</p> <p>These call-outs can also be made collapsible:</p> Info <p>This is a collapsed information call-out.</p> <p>We can also expand this collapsible call-out normally:</p> Warning <p>This is a collapsible warning call-out that is expanded by default.</p> <p>Mathematical equations can be added using the MathJax extension. More information can be found in the MathJax reference and the LaTex math reference.</p> <p>Formulas like \\(y(x)=2a*bx\\) can be added inline. Alternatively, mathematical equations can be added as a code block:</p> \\[   f = \\sum^{i=0}_{n}x_i*n_i+b^{i+1} \\] \\[   \\frac{1}{2}*\\sqrt(22a+16b^5) \\] <p>We can also add some diagrams using the Mermaid.js library. More info on the Mermaid.js examples or the Diagram reference</p> <pre><code>  graph LR\n    A((input 1)) -- weight 1 --&gt; B((neuron 1))\n    D((input 2)) -- weight 2 --&gt; B\n    B --&gt; E[Output]</code></pre> <ol> <li> <p>https://google.com\u00a0\u21a9</p> </li> </ol>"},{"location":"about/license/","title":"License","text":""},{"location":"about/license/#tinyspark-licenses","title":"TinySpark licenses","text":"<p>All content of the TinySpark project is licensed under CC BY-SA 4.0, the underlying source code is licensed under GNU AGPLv3.</p> <p>If you are interested in using (parts of) the contents on this platform, feel free to contact me via j-siderius@GitHub.</p> <p>Visit the original repository for more information: TinySpark/j-siderius@Github.</p> <p>\u00a9 2023 by JC Siderius</p>"},{"location":"about/project/","title":"Project","text":""},{"location":"about/project/#tinyspark","title":"TinySpark","text":"<p>The TinySpark project is created by J Siderius, as part of the Bachelor Thesis for Creative Technology in 2023.</p> <p>The projects' aim is to enable everyone, from hobbyist, maker, student to professor, to understand (Tiny) Machine Learning in a simple and intuitive way. Furthermore, by providing many sample projects, users should be able to integrate meaningful TinyML applications into their own projects and needs.</p>"},{"location":"beyond/introduction/","title":"Dive into TinyML","text":""},{"location":"beyond/introduction/#dive-into-tinyml","title":"Dive into TinyML","text":"<p>This chapter will be used to give some closing remarks, recommendations for further learning and project ideas to get started on your own TinyML powered projects.</p>"},{"location":"beyond/introduction/#closing-remarks","title":"Closing remarks","text":"<p>I hope that through the TinySpark platform and development kit, you have been able to lift the 'black box' that is often associated with machine learning (and subsequently TinyML). Through the step-by-step explanation of concepts and the mini-projects, you should have gained a better understanding of the mathematics and logic behind (Tiny) machine learning, as well as its application areas. While there are certainly many more nuances to be learned regarding machine learning, and deployment to TinyML capable devices, you should now have a solid basis to start from.</p>"},{"location":"beyond/introduction/#continue-learning-tinyml","title":"Continue learning TinyML","text":"<p>As could already be seen in the previous chapter, there are quite some calculations involved with the training and deployment of neural networks. As there is much mathematics involved, it can quickly become quite overwhelming and opaque again. If you plan on further developing more extensive neural networks using the techniques learned here, it is recommended to write a Python library or a class for handling all of the bookkeeping and calculations. Good starting points include the tutorial by D\u00e9borah Mesquita on Real Python, the tutorial by Dr. Michael J. Garbade on KDnuggets and the video series by Andrej Karpathy called Zero to AI Hero. </p> <p>Alternatively, open source libraries such as Tensorflow and PyTorch can be used to setup and train neural networks. These Python libraries support many more advanced features, and are maintained and updated regularly. For deploying models trained by Tensorflow, you can use the Tensorflow Lite Micro framework, that is very easily integrated into normal Tensorflow code. Specific examples that can be used on the TinySpark development kit can be found on the Espressif TFLite Micro examples page. Be aware that these machine learning frameworks produce neural networks that are not easily viewed or edited, as they are often deployed as compressed C code.</p> <p>To learn more about TinyML in general, the TinyML foundation hosts regular (online) talks with industry experts. Harvard University heads the Tiny Machine Learning Open Education Initiative which includes many resources from (free) courses, online tutorials, code repositories and much more. Lastly, the MIT HAN lab has interesting examples and explanations on the compression and acceleration of neural networks and models.</p>"},{"location":"beyond/introduction/#project-ideas","title":"Project ideas","text":"<p>If you want to continue and develop your own TinyML powered projects using the TinySpark development board, here are some project ideas:</p> <ul> <li>Vibration detection by analysing time series data from the Inertial Motion sensor</li> <li>Wake word detection using the on-board Microphone</li> <li>Weather change prediction using the Pressure sensor</li> <li>Game intelligence such as Tic-Tac-Toe or Snake</li> <li>Package handling analysis using the data from the Inertial Motion sensor</li> <li>Sentiment analysis from the pitch of voices using the Microphone</li> <li>Advanced plant monitoring using the Environmental sensor and Light sensor</li> <li>Infrared decoding of transmission signals using the on-board IR receiver</li> <li>Morse code decoding by time interval measurement</li> <li>Food quality prediction using the Environmental sensor</li> <li>Fall detection by analysing impact data from the Inertial Motion sensor</li> <li>Motor malfunction detection using the Hall effect sensor</li> </ul> <p>By connecting some external sensors using the Expansion header or Stemma QT / Qwiic connector, some more interesting projects could be made:</p> <ul> <li>Perfect al-dente pasta cooking through temperature measurement</li> <li>Pulse monitoring and fatigue detection using a pulse-oximeter</li> <li>Leak detection using flow sensing valves</li> <li>Navigational aid using LIDAR sensing</li> <li>Fruit ripeness detection by using a colour sensor</li> <li>Soil condition determination by measuring moisture and acidity</li> </ul>"},{"location":"beyond/introduction/#featured-projects","title":"Featured projects","text":"<p>If you have finished an interesting project and want it displayed here, please contact me via j-siderius@GitHub.</p>"},{"location":"chapter1/introduction/","title":"Introduction","text":""},{"location":"chapter1/introduction/#chapter-1-introduction-to-machine-learning","title":"Chapter 1 - Introduction to Machine Learning","text":"<p>A neural network is a computational model that mimics the structure and function of biological neurons. A neuron is a basic unit of a neural network that can receive, process and transmit information. In this chapter, neuron functions and connections will be explained. After the explanation, a simple project will be programmed to show how a neuron functions mathematically.</p> <p></p> <p>In the next section, the basics of neurons are introduced.</p>"},{"location":"chapter1/logic_gates/","title":"Logic gates","text":"<p>A logic gate (in electronics), is a device which performs logical operations using binary logic. Logic gates are the fundaments of modern processors, enabling chips to store data using Latches1, perform binary calculations using Arithmic Logic Units2 and much more.</p> <p>In this first Mini-project, a single-neuron 'network' will be built to replicate the behaviour of an OR logic gate3. The OR gate will activate whenever either of it's inputs reads HIGH (or 1 in this case).</p> <p></p> <p>The logic table for an OR gate looks like this:</p> Input  Output 0 0 0 0 1 1 1 0 1 1 1 1 <p>In order to make writing the equations easier, the sum and activation function are combined into one formula.</p> \\[ \\text{output}=f(\\sum\\text{inputs}*\\text{weights}) \\] <p>Now try if the neuron from the previous section will predict the correct output for the OR logic gate.</p> \\[ \\displaylines{ \\text{input 1}=0\\\\ \\text{input 2}=0\\\\ \\text{weight 1}=0.3\\\\ \\text{weight 2}=0.9\\\\ } \\] \\[ \\text{output}=f(0*0.3+0*0.9)=0 \\] <p>That looks promising, so the other inputs are tried as well.</p> \\[ \\displaylines{ \\text{output}=f(0*0.3+1*0.9)=1 \\\\ \\text{output}=f(1*0.3+0*0.9)=0 \\\\ \\text{output}=f(1*0.3+1*0.9)=1 \\\\ } \\] <p>Almost correct, however with the inputs \\(1,0\\), the neuron wrongly outputs \\(0\\), which should be \\(1\\) (according to the logic table above).</p> <p>How can this behaviour be changed?  By changing the weights.</p> <p>Take a closer look at the calculation of the incorrect output.</p> \\[ \\text{sum}=\\text{input 1}*\\text{weight 1}+\\text{input 2}+\\text{weight 2}\\\\ =1*0.3+0*0.9=0.3 \\] <p>Indeed, if the sum is run through the activation function, the result is incorrect.</p> \\[ f(0.3)=0 \\] <p>The output of the sum, \\(0.3\\), is too low to trigger the activation function. By intuitively approaching the calculations above, there are some things that can be concluded:</p> <ul> <li>the inputs cannot be changed</li> <li>the activation function could be changed</li> <li>the weights could be changed</li> </ul> <p>In neural networks, inputs cannot change, so it does not make sense to focus on that aspect of the calculation. The activation function could be changed, however since the activation function produced correct outputs for the other inputs, this is probably not the issue (additionally, activation functions in neural networks are only rarely changed to get to the desired output). Lastly, the weights can be changed, and consequently, this is the usual way of tuning a neural network to achieve correct outputs.</p> <p>In order to trigger the activation function \\(f(x)\\), the sum for inputs \\(1,0\\) needs to be higher than or equal to 0.5. If the \\(\\text{weight 1}\\) is increased to \\(0.5\\) for example, recalculate all outputs and see if the neuron 'network' is correct for all inputs.</p> \\[ \\displaylines{ \\text{output}=f(0*0.5+0*0.9)=0 \\\\ \\text{output}=f(0*0.5+1*0.9)=1 \\\\ \\text{output}=f(1*0.5+0*0.9)=1 \\\\ \\text{output}=f(1*0.5+1*0.9)=1 \\\\ } \\] <p>Alternatively, to see the changes \\(\\text{weight 1}\\) has on the outputs of the model, use the interactive visualisation below.</p> Weight 1 <p>Now that the weights are tuned correctly, program this neuron 'netowkr' into a simple Python script. To make sure that all possible inputs of the OR-gate are covered, they are stored inside of an array. The array will be looped over, and outputs for every input combination will be created.</p> <p></p> single_neuron_OR_gate.py<pre><code>inputs = [\n[0, 0],\n[0, 1],\n[1, 0],\n[1, 1]\n]\nweight1 = 0.5\nweight2 = 0.9\nfor input in inputs:\nsum = (input[0] * weight1) + (input[1] * weight2)\nif sum &gt;= 0.5:\nactivation = 1\nelse:\nactivation = 0\nprint(input, activation)\n</code></pre> <p>In the next section, this network will be deployted to the TinySpark development board, in order to experience how to expand the neuron 'network' beyond the screen.</p> <ol> <li> <p>https://en.wikipedia.org/wiki/Flip-flop_(electronics) \u21a9</p> </li> <li> <p>https://en.wikipedia.org/wiki/Arithmetic_logic_unit \u21a9</p> </li> <li> <p>https://en.wikipedia.org/wiki/OR_gate \u21a9</p> </li> </ol>"},{"location":"chapter1/logic_gates_micro/","title":"Logic gates on the TinySpark development board","text":"<p>Now that the first neuron 'network' was succesfully implemented, it will be transported to the TinySpark development kit.</p> <p>The two buttons, <code>Button 1</code> and <code>Button 2</code> on the development kit will be used to simulate the inputs, and thee LED (<code>LED13</code>) will be used to show if the output of the neuron 'network' is LOW/0 or HIGH/1. In order to access the Inputs and Outputs of the development kit, some code is needed. The code below combines the example code for interaction with the buttons and output through the LED.</p> <p></p> input_output.py<pre><code># import the library to take care of the pins\nimport board\nfrom digitalio import DigitalInOut, Direction, Pull\n# initialise the pins\nbutton1 = DigitalInOut(board.BUTTON1)\nbutton1.direction = Direction.INPUT\nbutton1.pull = Pull.UP\nbutton2 = DigitalInOut(board.BUTTON2)\nbutton2.direction = Direction.INPUT\nbutton2.pull = Pull.UP\nled = DigitalInOut(board.LED)\nled.direction = Direction.OUTPUT\n# turn the LED on\nled.value = True\n# loop endlessly\nwhile 1:\n# check if a button is pressed, and print if it is (buttons are pulled HIGH, so check for low)\nif not button1.value:\nprint(\"button1 pressed\")\nif not button2.value:\nprint(\"button2 pressed\")\n</code></pre> <p></p> <p>Now the logic from the previous section needs to be implemented, in order to complete the neuron 'network' and succesfully deploy it to the TinySpark development board.</p> <p></p> OR_gate.py<pre><code># import the library to take care of the pins\nimport board\nfrom digitalio import DigitalInOut, Direction, Pull\n# initialise the pins\nbutton1 = DigitalInOut(board.BUTTON1)\nbutton1.direction = Direction.INPUT\nbutton1.pull = Pull.UP\nbutton2 = DigitalInOut(board.BUTTON2)\nbutton2.direction = Direction.INPUT\nbutton2.pull = Pull.UP\nled = DigitalInOut(board.LED)\nled.direction = Direction.OUTPUT\n# store the weights\nweight1 = 0.5\nweight2 = 0.9\n# loop endlessly\nwhile 1:\n# Read the button value (buttons are pulled HIGH, so check for low)\ninput1 = button1.value\ninput2 = button2.value\nsum = (input1 * weight1) + (input2 * weight2)\nif sum &gt;= 0.5:\n# activation = 1\nled.value = True\nelse:\n# activation = 0\nled.value = False\n</code></pre> <p>Upload the code and see if the neuron 'network' functions as expected, by pressing <code>Button 1</code> and <code>Button 2</code> and observing the LED.</p> <p>In the next chapter, some more complicated predictions will be made by utilising an actual network of neurons.</p>"},{"location":"chapter1/neuron/","title":"The Neuron","text":"<p>The neuron is at the heart of every neural network, it provides all computation and links to other neurons and layers within the neural network. A neuron consists of three main components: an input layer, an activation function and an output layer. The input layer receives signals from other neurons or external sources, such as images, texts, or numbers. The activation function determines whether the neuron should fire or not, based on the input signals. The output layer sends the firing signal to other neurons or to the final output of the network.</p> <p></p> <p>The inputs of a neuron come in many different shapes and sizes (literally). A neuron could receive just one single input, or be connected to more than 500 different inputs. The inputs to a neuron are always numeric (since this is essentially one giant mathematical formula), so complex inputs like sound or images have to be split into many different parts (e.g. milliseconds of sound or pixels in an image). These can then be fed into the input of a network.</p> <p>The neuron with two inputs, two weights and a simple threshold activation function below will be examined. How do inputs travel through it and how do they lead to an output?</p> <p></p> <p>The first step in computation of the output is to sum the inputs of the neuron together with the weights of the neuron:</p> \\[ \\sum \\text{inputs}*\\text{weights} = \\text{input 1} * \\text{weight 1} + \\text{input 2} * \\text{weight 2} \\] <p>The activation function which comes after the inputs can be a simple threshold function that fires if the input signals exceed a certain value, or a more complex function that can capture nonlinear relationships between inputs and outputs. Some common activation functions are sigmoid, tanh, ReLU and softmax. More information can be found in the Machine Learning glossary.</p> <p>Now, the neuron needs to be activated by running the sum through the chosen activation function. In this example, a simple threshold function is chosen. If the sum of the neuron is \\(0.5\\) or higher, the output will be \\(1\\), otherwise the output will be \\(0\\).</p> \\[  f(x) = \\begin{cases}        0 &amp; \\text{if } x &lt; 0.5\\\\      1 &amp; \\text{if } x \\geq  0.5 \\end{cases} \\] <p>Now, the output of the neuron with these inputs and weights is calculated as follows:</p> \\[ \\displaylines{ \\text{input 1}=0.2\\\\ \\text{input 2}=0.8\\\\ \\text{weight 1}=0.3\\\\ \\text{weight 2}=0.9\\\\ } \\] \\[ \\sum \\text{inputs}*\\text{weights} = 0.2 * 0.3 + 0.8 * 0.9 = 0.78 \\\\ \\] \\[ \\text{output}=f(0.78)=1 \\] <p>The output of tje neuron, with the given inputs would be \\(1\\).</p> <p>If this is now implemented into a simple Python script, it could look like this:</p> <p>Implementing this simple neuron logic into Python should be straightforward.</p> <p></p> single_neuron.py<pre><code>input1 = 0.2\ninput2 = 0.8\nweight1 = 0.3\nweight2 = 0.9\nsum = (input1 * weight1) + (input2 * weight2)\nif sum &gt;= 0.5:\nactivation = 1\nelse:\nactivation = 0\nprint(activation)\n</code></pre> <p>In the next section, the neuron that was just created will be used to predict something more meaningful.</p>"},{"location":"chapter2/introduction/","title":"Chapter 2 - The neuron problem","text":"<p>As shown in the last chapter, a neuron is capable of predicting outcomes based on the inputs and weights it is given. A single neuron can only perform linear or simple non-linear transformations, which may not be sufficient to represent the complexity of real-world problems. To overcome this restriction, a network of neurons can model non-linear relationships better than a single neuron, because it can capture more features and interactions among the inputs.</p> <p>The network component of a neural network is the structure that defines how the neurons are arranged and connected. A neural network consists of multiple layers of neurons, each layer performing some computation on the inputs from the previous layer and passing the outputs to the next layer. The first layer is called the input layer, and the last layer is called the output layer. The layers in between are called hidden layers. The number and size of the hidden layers determine the complexity and capacity of the neural network. In this chapter, a closer look will be taken at the network that forms prediction models.</p> <p></p>"},{"location":"chapter2/network_connections/","title":"Network connections","text":"<p>Looking back at a single neuron, which has an input, weight, activation function and output, it is easy to see how one could chain neurons together to form a network.</p> <p></p> <p>By connecting the output of a neuron to the input of another neuron, the beginning of a network is formed. Extending this to more neurons, for example, two input neurons, two hidden neurons and one output neuron. If all neurons are interconnected to each other, like in the image below, this is called a Fully-connected Neural Network.</p> <p></p> <p>Input neurons or nodes generally have no activation function and weights associated to them, they merely provide a place to store inputs for further computation.</p> <p>In the next section, the Fully-connected Neural Network in the image above will be implemented in Python.</p>"},{"location":"chapter2/plant_monitoring/","title":"Plant monitoring","text":"<p>In the first chapter, Logic gates were introduced. The mini-project programmed an OR gate in Python, and later implemented it onto the TinySpark development board.</p> <p>During this mini-project, plant care is of highest regard. The Fictioplantus1, as can be read below, is a very delicate plant from the Botanica Kingdom. It requires careful control of both temperature and humidity, as it will grow very poorly when conditions are not right.</p> <p></p> <p>The requirements for succesful growth are very particular, they are summarized in the table below.</p> Temperature Humidity Growth below 25\u00b0C below 70% no above 25\u00b0C above 70% no below 25\u00b0C above 70% yes above 25\u00b0C below 70% yes <p>Classifying inputs like the temperature and humidity is more difficult than classifying simple logic gates. The reason behind this is the case of linear and non-linear separability2. Separability refers to the property of a dataset or set of points (in this case the inputs) where it is possible to draw a straight line that can completely separate the points into different classes (below/above 25\u00b0 or below/above 70% humidity in this case). The problem proposed above is such a non-linearly separable problem. To overcome this, it is nescessary to introduce more neurons into the neural network.</p> <p></p> <p>The network from the previous section is used here. Weights are defined as seen. For the activation function, the step function from Chapter 1 is used again. Note how in this example, negative weights are also possible in order to give inputs negative influences as well.</p> \\[  f(x) = \\begin{cases}        0 &amp; \\text{if } x &lt; 0.5\\\\      1 &amp; \\text{if } x \\geq  0.5 \\end{cases} \\] \\[ \\displaylines{ \\text{weight 1}=0.3\\\\ \\text{weight 2}=-0.5\\\\ \\text{weight 3}=-0.6\\\\ \\text{weight 4}=0.4\\\\ \\text{weight 5}=0.7\\\\ \\text{weight 6}=0.5\\\\ } \\] <p>In order to process the inputs correctly, some pre-proccessing of the measurements needs to take place. This is done in order to make calculations easier, and to keep weights managable (in this case within ranges -1 and 1). The calculation is described below. In complicated neural networks, this pre-processing can be part of the network, in a so called Convolution step3.</p> \\[ \\displaylines{ temp_{in} = (temperature - 25) / 10\\\\ humid_{in} = (humidity - 70) / 10\\\\ } \\] <p>Calculating the outputs for some possible input combinations is now more complicated than in the previous chapter. As inputs, a few values above and below the threshold are chosen: \\(22\\)\u00b0C and \\(30\\)\u00b0C for temperature, \\(40\\)% and \\(85\\)% for humidity. In the calculations below, first the pre-processing of the values is done, then the respective outputs are calculated. Remember that because the activation function used is still a step function, the output will always be either \\(0\\) or \\(1\\) (signifying poor and good growing conditions respectively).</p> \\[ \\displaylines{ \\text{input(22\u00b0C)}=(22 - 25) / 10 = -0.3\\\\ \\text{input(30\u00b0C)}=(30 - 25) / 10 = 0.5\\\\ \\text{input(40%)}=(40 - 70) / 10 = -3\\\\ \\text{input(80%)}=(80 - 70) / 10 = 1\\\\ } \\] Network calculation results <p>For the weights above, the calculated predictions for the network can be found below. </p> \\[ \\displaylines{ \\text{output(-0.3, -3)}=f(f(-0.3*0.3+-3*-0.5)*0.7+f(-0.3*-0.6+-3*0.4)*0.5)=0\\\\ \\text{output(-0.3, 1)}=f(f(-0.3*0.3+1*-0.5)*0.7+f(-0.3*-0.6+1*0.4)*0.5)=1\\\\ \\text{output(0.5, -3)}=f(f(0.5*0.3+-3*-0.5)*0.7+f(0.5*-0.6+-3*0.4)*0.5)=1\\\\ \\text{output(0.5, 1)}=f(f(0.5*0.3+1*-0.5)*0.7+f(0.5*-0.6+1*0.4)*0.5)=0\\\\ } \\] <p>Using the interactive visualisation below, try to tune the weights so that the prediction is correct for the given inputs. The weights given above can be used as guidance, however there are many different possible combinations of weights to be found that will lead to the desired output. Note that this tuning is meant more as a exercise to see what effects weights have in a neural network. It is expected to be very difficult to find correct weights by hand.</p> Weight 1 Weight 2 Weight 3 Weight 4 Weight 5 Weight 6 Temperature Humidity <p>Now program the found weights (of the pre-given ones) into a simple Python script. The weights of the network will be stored inside of an array. The inputs for temperature and humidity can be either input manually, or fetched from an external API4 that supplies weather data, such as the Dutch weather forecast Buienradar API.</p> <p></p> plant_monitoring.py<pre><code># import the libraries to fetch weather data from an API\nimport requests\nimport json\n# store the weights\nweights = [\n0.3,\n-0.5,\n-0.6,\n0.4,\n0.7,\n0.5\n]\n# defining the activation function\ndef activation(x):\nif x &gt;= 0.5:\nreturn 1\nelse:\nreturn 0\n# formulate the API request\nresponse = requests.get('https://data.buienradar.nl/2.0/feed/json')\n# get the temperature and humidity from the Buienradar API\n# the location is currently set to De Bilt in the Netherlands\ntemperature = response.json()['actual']['stationmeasurements'][3]['temperature']\nhumidity = response.json()['actual']['stationmeasurements'][3]['humidity']\n# Alternatively, input the temperature and humidity manually\n# temperature = 23\n# humidity = 65\n# Print the inputs\nprint(f\"Temperature: {temperature}\u00b0C, Humidity: {humidity}%\")\n# pre-processing the inputs\ntemp_in = (temperature - 25) / 10\nhumid_in = (humidity - 70) / 10\n# perform network calculations\nneuron1 = activation( (temp_in * weights[0]) + (humid_in * weights[1]) )\nneuron2 = activation( (temp_in * weights[2]) + (humid_in * weights[3]) )\noutput = activation( (neuron1 * weights[4]) + (neuron2 * weights[5]) )\n# printing the result\nif output == 1:\nprint(\"Growing conditions are good\")\nelse:\nprint(\"Growing conditions are poor\")\n</code></pre> <p>In the next section, the network will be deployed to the TinySpark development board, utilising the on-board environmental sensor.</p> <ol> <li> <p>The Fictioplantus acts as a simplified plant example in this case, although principles learned in this project can be applied to real plant monitoring.\u00a0\u21a9</p> </li> <li> <p>https://en.wikipedia.org/wiki/Linear_separability \u21a9</p> </li> <li> <p>https://en.wikipedia.org/wiki/Layer_(deep_learning) \u21a9</p> </li> <li> <p>https://en.wikipedia.org/wiki/API \u21a9</p> </li> </ol>"},{"location":"chapter2/plant_monitoring_micro/","title":"Plant monitoring on the TinySpark development board","text":"<p>Now that you have implemented your first real neural network, deploy the model to the TinySpark development kit.</p> <p>The environmental sensor (more information on the TinyML development kit sensors page) will be used for the temperature and humidity measurements. In a deployment, this sensor could be mounted close to the plant, and be configured to send a message to the gardener, or remotely open a window or start a ventilator. For now, the result of the prediction will be printed to the Serial monitor (see the Programming guide for more information).</p> <p></p> <p>Implement the logic from the previous section, into the TinySpark development board, using the Environmental sensor example code.</p> <p></p> plant_monitoring.py<pre><code># import the libraries to take care of the sensor and manage time\nimport board\nimport time\nfrom adafruit_bme280 import basic as adafruit_bme280\n# initialise the environmental sensor\ni2c = board.I2C()\nbme280 = adafruit_bme280.Adafruit_BME280_I2C(i2c, address=0x76)\n# store the weights\nweights = [\n0.3,\n-0.5,\n-0.6,\n0.4,\n0.7,\n0.5\n]\n# defining the activation function\ndef activation(x):\nif x &gt;= 0.5:\nreturn 1\nelse:\nreturn 0\n# loop endlessly\nwhile 1:\n# get the sensor readings and print them\ntemperature = bme280.temperature\nhumidity = bme280.relative_humidity\nprint(f\"Temperature: {temperature}, Humidity: {humidity}\")\n# pre-processing the inputs\ntemp_in = (temperature - 25) / 10\nhumid_in = (humidity - 70) / 10\n# perform network calculations\nneuron1 = activation( (temp_in * weights[0]) + (humid_in * weights[1]) )\nneuron2 = activation( (temp_in * weights[2]) + (humid_in * weights[3]) )\noutput = activation( (neuron1 * weights[4]) + (neuron2 * weights[5]) )\n# printing the result\nif output == 1:\nprint(\"Growing conditions are good\")\nelse:\nprint(\"Growing conditions are poor\")\n# perform the prediction every 2 seconds to see if it has changed\ntime.sleep(2)\n</code></pre> <p>Now upload the code and see if the network works on the development board. You can try to manipulate the readings for temperature and humidity by breathing or blowing on the environmental sensor (see where it is on the board in the TinyML kit introduction). To cool the sensor down again or to get the humidity lower, let some ambient air go over the sensor, for example by holding it in the wind or by putting a fan over it.</p> <p>In the next chapter, a neural network will be mathematically trained, and a program is written to repeat this many times, making it possible to create more accurate and larger networks.</p>"},{"location":"chapter3/gesture_recognition_data/","title":"Gesture recognition - data aquisition","text":"<p>In this mini-project, a simple gesture recognition system will be built. Using the on-board proximity sensor introduced in the TinySpark development section, two states will be detected: </p> <ul> <li>moving closer</li> <li>moving away</li> </ul> <p></p> <p>To detect these gestures, a network that takes in multiple proximity readings needs to be devised. In this example, three measurements will be input into the network: one measurement that is current, one that was 250ms ago, and one that is 500ms ago. This Time series1 prediction method is a common way to analyse real-time sensor data using a neural network.</p> <p>In order to train an accurate neural network, it is important to record actual data on the end device. In more complex systems however, it is not feasible to train the neural network on the data recording device, as it would take far too long to perform all training calculations there. Therefore, it is common practise to capture measurements on the local device, transfer them to a more powerful computer (e.g. PC or even to the cloud) and perform the training there. Afterwards, only the tuned weights and the feedforward part of the neural network are transfered back to the local device in order to make predictions.</p> <p></p> <p>To start the mini-project, some data needs to be recorded on the TinySpark development kit. The code below will start the recording of the datapoints one second after <code>Button 1</code> is pressed. The LED will show when a recording is made. After the recording has finished, the values will be stored in an array. Once <code>Button 2</code> is pressed, all datapoints are printed onto the serial console. For information on using the serial console, please read the TinySpark Programming section. From there, it is possible to copy the measurements over to a training program, which will be discussed further along this section.</p> <p></p> gesture_data_recording.py<pre><code># Import all libraries\nimport time\nimport board\nfrom apds9930.apds9930 import APDS9930\nfrom digitalio import DigitalInOut, Direction, Pull\n# Initialize I2C\ni2c = board.I2C()\nsensor = APDS9930(i2c)\n# Initialise buttons 1 and 2\nbutton1 = DigitalInOut(board.BUTTON1)\nbutton1.direction = Direction.INPUT\nbutton1.pull = Pull.UP\nbutton2 = DigitalInOut(board.BUTTON2)\nbutton2.direction = Direction.INPUT\nbutton2.pull = Pull.UP\n# Initialise LED\nled = DigitalInOut(board.LED)\nled.direction = Direction.OUTPUT\n# Array for storing the measurements\nproximity_readings = list()\n# Loop continuously\nwhile True:\n# Check button 1 (note the signal is pulled up / high, so check for low signal)\nif button1.value == 0:\n# Startup delay\nrecording = list()\nprint(\"&gt; Starting recording in 1sec\")\ntime.sleep(1)\n# Start recording\nled.value = 1\ndistance = sensor.proximity\nrecording.append(distance)\nprint(f\"{distance=}\")\n# delay for 250ms\ntime.sleep(0.250)\ndistance = sensor.proximity\nrecording.append(distance)\nprint(f\"{distance=}\")\n# delay for another 250ms\ntime.sleep(0.250)\ndistance = sensor.proximity\nrecording.append(distance)\nprint(f\"{distance=}\")\n# Stop recording\nled.value = 0\nprint(\"&gt; Stopped recording\")\nproximity_readings.append(recording)\n# Check button 2\nelif button2.value == 0:\n# Print all recordings to the serial console, in form of an array\nprint(\"Recordings: [-500ms, -250ms, current]\")\nprint(\"[\")\nfor recording in proximity_readings:\nprint(f\"[{recording[0]}, {recording[1]}, {recording[2]}],\")\nprint(\"]\")\n# Clear the measurement storage\nproximity_readings = list()\n# Pause after resetting\ntime.sleep(1)\n# Debounce the buttons\ntime.sleep(0.01)\n</code></pre> <p>To record some datapoints, press <code>Button 1</code> and when the LED turns on, then perform a gesture. After recording some datapoints, press <code>Button 2</code> to print them to the serial console. It is advisable to record all datapoints for one type of gesture, then print those, then continue to record the next type. Store the measurements somewhere (as they will be used in the next section) and make sure to label them appropriately. About 5-7 datapoints per gesture are enough for this example.</p> Proximity sensor measurements <p>Since the proximity sensor included on the TinySpark development board was originally produced for use in hand-detecting applications such as automatic soap dispensers, the optimal measurement range is between 10-25cm from the sensor. Additionally, it is easiest to use a solid coloured object such as a piece of paper or cardboard to record the gestures, as this ensures accurate detection.</p> <p>Use the distance printed to the serial console in order to see if the measurement(s) makes sense. If they don't seem right, try again and delete the old measurement(s) by emptying the array using <code>Button 2</code>. Also look at TinySpark development kit description to see the example code for the proximity (and light) sensor.</p> <p>In the next section, the measurements will be run through the neural network and the weights will be trained using backpropagation.</p> <ol> <li> <p>https://en.wikipedia.org/wiki/Time_series \u21a9</p> </li> </ol>"},{"location":"chapter3/gesture_recognition_deploy/","title":"Gesture recognition - deploying the model","text":"<p>Now it is time to deploy the trained model to the TinySpark development board. In order to do so, the recording code from a previous section is rewritten to accomodate the prediction. The trained weights are included in the code. Since the training phase of the neural network is not needed anymore, its code will not be included from the previous section. The program will work by pressing <code>Button 1</code> and then recording a gesture. The on-board LED will be on when recording the gesture. The final prediction (the gesture with the highest probability) is printed to the serial console together with its probability.</p> <p></p> gesture_model.py<pre><code># Import all libraries\nimport time\nimport board\nfrom apds9930.apds9930 import APDS9930\nfrom digitalio import DigitalInOut, Direction, Pull\n# Initialize I2C\ni2c = board.I2C()\nsensor = APDS9930(i2c)\n# Initialise buttons 1 and 2\nbutton1 = DigitalInOut(board.BUTTON1)\nbutton1.direction = Direction.INPUT\nbutton1.pull = Pull.UP\nbutton2 = DigitalInOut(board.BUTTON2)\nbutton2.direction = Direction.INPUT\nbutton2.pull = Pull.UP\n# Initialise LED\nled = DigitalInOut(board.LED)\nled.direction = Direction.OUTPUT\n### Put the trained weights here ###\nweights=[[0.664838063268332, 0.14475863419970098, 0.39798479496743655, -0.1996792640967368, -0.10042784907697525, -0.38665016231765525], [0.19784347834163832, -0.7106415457739413, 0.6545148412015216, 0.1258938146460059]]\n# Define the activation function (linear)\ndef activation(x):\nreturn x\n# Loop continuously\nwhile True:\n# Check button 1 (note the signal is pulled up / high, so check for low signal)\nif button1.value == 0:\n# Startup delay\nrecording = list()\nprint(\"&gt; Starting recording in 1sec\")\ntime.sleep(1)\n# Start recording\nled.value = 1\ndistance = sensor.proximity\nrecording.append(distance)\nprint(f\"{distance=}\")\n# delay for 250ms\ntime.sleep(0.250)\ndistance = sensor.proximity\nrecording.append(distance)\nprint(f\"{distance=}\")\n# delay for another 250ms\ntime.sleep(0.250)\ndistance = sensor.proximity\nrecording.append(distance)\nprint(f\"{distance=}\")\n# Stop recording\nled.value = 0\nprint(\"&gt; Stopped recording\")\n# Start predicting\ninputs = [r/100 for r in recording]\n# Calculate the output of the hidden layer\nhiddens = [\nactivation(inputs[0] * weights[0][0] + inputs[1] * weights[0][1] + inputs[2] * weights[0][2]),\nactivation(inputs[0] * weights[0][3] + inputs[1] * weights[0][4] + inputs[2] * weights[0][5])\n]\n# Calculate the output from the output layer\noutputs = [\nactivation(hiddens[0] * weights[1][0] + hiddens[1] * weights[1][1]),\nactivation(hiddens[0] * weights[1][2] + hiddens[1] * weights[1][3]),\n]\n# Print the prediction, keeping in mind that the output with the highest probability is picked\nif outputs[0] &gt;= outputs[1]:\nprint(f\"Prediction: moving closer, p={outputs[0]}\")\nelse:\nprint(f\"Prediction: moving away, p={outputs[1]}\")\n# Debounce the buttons\ntime.sleep(0.01)\n</code></pre> <p>Deploy the model to the TinySpark development kit and add your trained weights to it. Press <code>Button 1</code> and see if the model predicted the gesture correctly.</p> Deployment problems <p>The trained model might behave differently than expected once it has been uploaded to the TinySpark development board. This could be due to a plethora of reasons, some are listed below.</p> <ol> <li>The measurement taken was not performed in the same way as the recordings during training (e.g. using a piece of cardboard vs your hand).</li> <li>The measurement taken was not performed in the same lighting conditions as the recordings during training (e.g. inside a dimly lit room vs outside).</li> <li>The model was trained using a narrow range of measurements (all measurements resembled each other very closely), this can lead to a skewed prediction model.</li> <li>The model was trained with too little samples.</li> </ol> <p>In the next chapter, there will be some closing remarks, recommendations for further learning and project ideas to get you started on your own TinyML journey.</p>"},{"location":"chapter3/gesture_recognition_training/","title":"Gesture recognition - training the model","text":"<p>To continue building the neural network that will recognise gestures, the mathemathics as discussed in a previous section will be implemented in Python. Then the aquired data from the last section will be used to train the network. Finally, the tuned weights will be used to run the model on the TinySpark development kit in order to detect gestures.</p> <p>For this model, a neural network with two hidden neurons and two output neurons is chosen. Since three measurements are taken, three input neurons are required. For the output, two neurons are chosen, as to give the probability of each of the gestures: \\(\\text{output1}=p(\\text{moving closer})\\) and \\(\\text{output2}=p(\\text{moving away})\\). The weights are initialised randomly, since it is not possible to determine starting weights by hand in a meaningful way anymore. The randomisation uses a uniform random distribution between \\(-1\\) and \\(1\\). Below, a depiction of the described neural network can be found.</p> <p></p> <p>With the measurements taken in the previous section, one example calculation will be performed on a moving closer measurement. All variables that belong together will be grouped into an array for convenience. Additionally, the linear activation function is defined.</p> \\[ \\displaylines{ \\text{measurement}=[5.1, 46.7, 120.5]\\\\ \\text{expected output}=[1, 0]\\\\ \\text{weights}1=[-0.50, 0.22, -0.91, 0.61, 0.14, -0.48]\\\\ \\text{weights}2=[0.73, 0.29, -0.29, -0.24]\\\\ f(x)=x\\\\ } \\] <p>Since the measurement values are quite large, and it is best to keep the weights in a network within managable ranges (e.g. between \\(-2\\) and \\(2\\)), some pre-processing of the data needs to be performed again. For each measurement in the measurements array, the following calculation will be performed</p> \\[ measurement_{in} = \\frac{measurement}{100} \\] \\[ \\displaylines{ \\text{measurement}=[5.1, 46.7, 120.5]\\\\ \\text{measurement}_{in}=[0.051, 0.467, 1.205]\\\\ } \\] <p>The hidden layer can now be determined by using the feedforward calculation.</p> \\[ \\displaylines{ \\text{hidden}1=f(\\text{measurement}_{in}1 * \\text{weights}11 + \\text{measurement}_{in}2 * \\text{weights}12 + \\text{measurement}_{in}3 * \\text{weights}13)\\\\ \\text{hidden}2=f(\\text{measurement}_{in}1 * \\text{weights}14 + \\text{measurement}_{in}2 * \\text{weights}15 + \\text{measurement}_{in}3 * \\text{weights}16)\\\\ \\text{output}1=f(\\text{hidden}1 * \\text{weights}21 + \\text{hidden}2 * \\text{weights}22)\\\\ \\text{output}2=f(\\text{hidden}1 * \\text{weights}23 + \\text{hidden}2 * \\text{weights}24)\\\\ } \\] \\[ \\displaylines{ \\text{hidden}1=-1.0193\\\\ \\text{hidden}2=-0.4819\\\\ \\text{output}1=-0.8839\\\\ \\text{output}2=0.4112\\\\ } \\] <p>Now the error is calculated. Additionally, since there is more than one output, the Loss is calculated. This serves as a measure of all error in the system (the compound error). There are many different loss functions1, but for the sake of simplicity, the loss function will just be a summation of the errors.</p> \\[ \\displaylines{     \\text{error}1=\\text{output}1 - \\text{expected output}1 = -0.8839 - 1 = -1.8839\\\\     \\text{error}2=\\text{output}2 - \\text{expected output}2 = 0.4112 - 0 = 0.4112\\\\     \\text{loss}=\\text{error}1 + \\text{error}2 = -1.4723 } \\] <p>To now make the model more accurate, backpropagation will be performed. First, the deltas of the output layer weights (\\(\\text{weights}2\\)) will be calculated. This is again done by going 'backwards' and using the derivatives, as shown in one of the previous sections. Recall that the derivative of \\(f(x)\\) was \\(1\\) and the derivative of the \\(\\text{sum}_{weights}\\) was the respective input.</p> \\[ \\displaylines{ \\delta_{weights}21=\\text{error}1*f'(x)*\\text{sum}'_{weights21}=-1.8893*1*-1.0193=1.9258\\\\ \\delta_{weights}22=\\text{error}1*f'(x)*\\text{sum}'_{weights22}=-1.8893*1*-0.4819=0.9105\\\\ \\delta_{weights}23=\\text{error}2*f'(x)*\\text{sum}'_{weights23}=0.4116*1*-1.0193=-0.4195\\\\ \\delta_{weights}24=\\text{error}2*f'(x)*\\text{sum}'_{weights24}=0.4116*1*-0.4819=-0.1984\\\\ } \\] <p>In order to calculate deltas for the weights in the hidden layer, one more step needs to be considered. Since the weights of the hidden layer neurons affect not only the outcome of their own neuron, but also the outcome of neurons further 'downstream' to them (e.g. in this case the output neurons), their total influence needs to be taken into account when calculating the delta. In practise, this means that first, the backpropagation of the weight for \\(\\text{output}1\\) needs to be calculated, and then the backpropagation of the weight for \\(\\text{output}2\\) is calculated. Subsequently, these values are summed. In larger networks, with more hidden layers, these calculations can become pretty big and almost impossible to write down in formulas. That is why many machine learning libraries (such as Tensorflow2) do all of this math in the background. But now, for the network with a single hidden layer, the formula will be worked out fully.</p> Practical thinking about backpropagation <p>If it is conceptually hard to understand how the backpropagation might function in a larger network, it is best to draw (imaginary) lines from the current delta / weight you are trying to calculate to every possible output of the model. These lines now show which backpropagation calculations need to be performed and summed to get the final delta.</p> \\[ \\displaylines{ \\delta_{weights}11=\\text{error}1*f'(x)*\\text{sum}'_{weights21}*f'(x)*\\text{sum}'_{weights11} + \\text{error}2*f'(x)*\\text{sum}'_{weights23}*f'(x)*\\text{sum}'_{weights11}\\\\ \\delta_{weights}12=\\text{error}1*f'(x)*\\text{sum}'_{weights21}*f'(x)*\\text{sum}'_{weights12} + \\text{error}2*f'(x)*\\text{sum}'_{weights23}*f'(x)*\\text{sum}'_{weights12}\\\\ \\delta_{weights}13=\\text{error}1*f'(x)*\\text{sum}'_{weights21}*f'(x)*\\text{sum}'_{weights13} + \\text{error}2*f'(x)*\\text{sum}'_{weights23}*f'(x)*\\text{sum}'_{weights13}\\\\ \\delta_{weights}14=\\text{error}1*f'(x)*\\text{sum}'_{weights22}*f'(x)*\\text{sum}'_{weights14} + \\text{error}2*f'(x)*\\text{sum}'_{weights24}*f'(x)*\\text{sum}'_{weights14}\\\\ \\delta_{weights}15=\\text{error}1*f'(x)*\\text{sum}'_{weights22}*f'(x)*\\text{sum}'_{weights15} + \\text{error}2*f'(x)*\\text{sum}'_{weights24}*f'(x)*\\text{sum}'_{weights15}\\\\ \\delta_{weights}16=\\text{error}1*f'(x)*\\text{sum}'_{weights22}*f'(x)*\\text{sum}'_{weights16} + \\text{error}2*f'(x)*\\text{sum}'_{weights24}*f'(x)*\\text{sum}'_{weights16}\\\\ } \\] \\[ \\displaylines{ \\delta_{weights}11=-1.8893*1*-1.0193*1*0.051 + 0.4116*1*-1.0193*1*0.051 = 0.0768\\\\ \\delta_{weights}12=-1.8893*1*-1.0193*1*0.467 + 0.4116*1*-1.0193*1*0.467 = 0.7034\\\\ \\delta_{weights}13=-1.8893*1*-1.0193*1*1.205 + 0.4116*1*-1.0193*1*1.205 = 1.8150\\\\ \\delta_{weights}14=-1.8893*1*-0.4819*1*0.051 + 0.4116*1*-0.4819*1*0.051 = 0.0363\\\\ \\delta_{weights}15=-1.8893*1*-0.4819*1*0.467 + 0.4116*1*-0.4819*1*0.467 = 0.3326\\\\ \\delta_{weights}16=-1.8893*1*-0.4819*1*1.205 + 0.4116*1*-0.4819*1*1.205 = 0.8580\\\\ } \\] <p>Now that all deltas are calculated, they need to be applied to the weights. As introduced in a previous section, it is often common to multiply the delta to a learning rate, in order to limit the influence of any single measurement (sample) on the model. In this example, a learning rate of 0.01 will be applied. While this may seem small for this single calculation, keep in mind that these training calculations are often performed hundreds or even thousands of times.</p> \\[ \\displaylines{ \\text{weight11}_{new}=\\text{weight11}-(\\text{learning rate}*\\delta_{weight11})=-0.50-(0.01*0.0768)=-0.5008\\\\ \\text{weight12}_{new}=\\text{weight12}-(\\text{learning rate}*\\delta_{weight12})=0.22-(0.01*0.7034)=0.2130\\\\ \\text{weight13}_{new}=\\text{weight13}-(\\text{learning rate}*\\delta_{weight13})=-0.91-(0.01*1.8150)=-0.9282\\\\ \\text{weight14}_{new}=\\text{weight14}-(\\text{learning rate}*\\delta_{weight14})=0.61-(0.01*0.0363)=0.6096\\\\ \\text{weight15}_{new}=\\text{weight15}-(\\text{learning rate}*\\delta_{weight15})=0.14-(0.01*0.3326)=0.1367\\\\ \\text{weight16}_{new}=\\text{weight16}-(\\text{learning rate}*\\delta_{weight16})=-0.48-(0.01*0.8580)=-0.4886\\\\ } \\] \\[ \\displaylines{ \\text{weight21}_{new}=\\text{weight21}-(\\text{learning rate}*\\delta_{weight21})=0.73-(0.01*1.9258)=0.7107\\\\ \\text{weight22}_{new}=\\text{weight22}-(\\text{learning rate}*\\delta_{weight22})=0.29-(0.01*0.9105)=0.2809\\\\ \\text{weight22}_{new}=\\text{weight23}-(\\text{learning rate}*\\delta_{weight23})=-0.29-(0.01*-0.4195)=-0.2858\\\\ \\text{weight23}_{new}=\\text{weight24}-(\\text{learning rate}*\\delta_{weight24})=-0.24-(0.01*-0.1984)=-0.238\\\\ } \\] <p>Now that the weights have been tuned, check if the prediction error or loss has improved.</p> \\[ \\displaylines{ \\text{hidden}1=-1.0446\\\\ \\text{hidden}2=-0.4938\\\\ \\text{output}1=-0.8810\\\\ \\text{output}2=0.4161\\\\ } \\] \\[ \\displaylines{ \\text{error1}_{new}=\\text{output}1 - \\text{expected output}1 = -0.8810 - 1 = -1.8810\\\\ \\text{error2}_{new}=\\text{output}2 - \\text{expected output}2 = 0.4161 - 0 = 0.4161\\\\ \\text{loss}_{new}=\\text{error}1 + \\text{error}2 = -1.4649 } \\] <p>The initial error was \\(-1.4723\\) and the new error has reduced to \\(-1.4649\\), meaning that the training was succesful. Now the above steps need to be reproduced many times, and with different measurements, to ensure that the resulting model works for both gestures.</p> <p>After manually working out one measurement training cycle, it may become apparent why it is good to let a computer program handle all the calculations. So, the training of the gesture recognition system will be put into a Python program.</p> <p>To introduce some variance into the training, the measurements / samples are shuffled for each training cycle. This ensures that the model gets trained evenly and unbiased. The amount of training epochs and the learning rate were chosen after some experimentation. It is interesting to see what changes in the calculation if these are tweaked (when using machine learning libraries, it is also common to tune these parameters until the desired output or loss is achieved). See if the model is able to train successfully on the measurements recorded in the last section by adding your own measurements into the code.</p> <p></p> training_model.py<pre><code># Import the random library to initialise the weights\nimport random\n### Put the recorded measurements here ### \nmoving_closer = [\n[0.0, 35.5, 120.1],\n[2.625, 33.375, 127.875],\n[19.0, 56.375, 127.875],\n[47.875, 51.375, 127.875],\n[40.125, 89.375, 127.875]\n]\nmoving_away = [\n[104.54, 2.375, 0.0],\n[127.875, 33.625, 5.4],\n[120.3, 39.125, 0.0],\n[127.875, 103.25, 23.8],\n[127.875, 29.25, 0.0]\n]\n# Labeling the collected data, outputting the probability of that gesture\n# output1 = p(moving closer), output2 = p(moving away)\nmoving_closer_labeled = [(sample, [1, 0]) for sample in moving_closer]\nmoving_away_labeled = [(sample, [0, 1]) for sample in moving_away]\n# Making one pile of samples from the split samples above\nsamples = moving_closer_labeled + moving_away_labeled\n# Initialise the weights for the network randomly, define the learning rate and epochs (itterations)\nweights = [\n[random.uniform(-0.1, 0.1) for _ in range(6)],\n[random.uniform(-0.1, 0.1) for _ in range(4)]\n]\nlearning_rate = 0.01\nepochs = 100\n# Define the activation function (linear)\ndef activation(x):\nreturn x\n# Run the training for the configured epochs\nfor epoch in range(epochs):\n# Shuffle the samples in order to introduces some variance into the training\nrandom.shuffle(samples)\n# Define a total loss for the whole system\nloss = 0.0\n# Run through all samples\nfor sample in samples:\n# Separate the inputs and expected outputs from the sample, pre-processing the inputs\ninputs = [s/100 for s in sample[0]]\nexpected = sample[1]\n# Calculate the output of the hidden layer\nhiddens = [\nactivation(inputs[0] * weights[0][0] + inputs[1] * weights[0][1] + inputs[2] * weights[0][2]),\nactivation(inputs[0] * weights[0][3] + inputs[1] * weights[0][4] + inputs[2] * weights[0][5])\n]\n# Calculate the output from the output layer\noutputs = [\nactivation(hiddens[0] * weights[1][0] + hiddens[1] * weights[1][1]),\nactivation(hiddens[0] * weights[1][2] + hiddens[1] * weights[1][3]),\n]\n# Calculate the errors\nerrors = [\noutputs[0] - expected[0],\noutputs[1] - expected[1]\n]\n# Add the errors to the system loss\nloss += errors[0] + errors[1]\n# Calculate the deltas for each weight\ndeltas = [\n[\nerrors[0] * 1 * hiddens[0] * 1 * inputs[0] + errors[1] * 1 * hiddens[0] * 1 * inputs[0],\nerrors[0] * 1 * hiddens[0] * 1 * inputs[1] + errors[1] * 1 * hiddens[0] * 1 * inputs[1],\nerrors[0] * 1 * hiddens[0] * 1 * inputs[2] + errors[1] * 1 * hiddens[0] * 1 * inputs[2],\nerrors[0] * 1 * hiddens[1] * 1 * inputs[0] + errors[1] * 1 * hiddens[1] * 1 * inputs[0],\nerrors[0] * 1 * hiddens[1] * 1 * inputs[1] + errors[1] * 1 * hiddens[1] * 1 * inputs[1],\nerrors[0] * 1 * hiddens[1] * 1 * inputs[2] + errors[1] * 1 * hiddens[1] * 1 * inputs[2],\n],\n[\nerrors[0] * 1 * hiddens[0],\nerrors[0] * 1 * hiddens[1],\nerrors[1] * 1 * hiddens[0],\nerrors[1] * 1 * hiddens[1]\n]\n]\n# Apply the calculated deltas to the weights, keeping in mind the learning rate\nfor layer in range(len(weights)):\nfor weight in range(len(weights[layer])):\nweights[layer][weight] -= learning_rate * deltas[layer][weight]\n# Print the loss every 10 epochs in order to check progress\nif epoch % 10 == 0:\nprint(f\"{epoch:} {loss=}\")\n# Print the final loss of the system and the weights\nprint(f\"{loss=}\")\nprint(f\"{weights=}\")\n</code></pre> Training problems <p>Training a neural network needs insights and sometimes even luck. Below are some points of attention to look at when the training does not work like intended.</p> <ol> <li>One or more of the weights of the model have exploded (gone far into the negative or positive), subsequently skewing the whole model. This can be solved by reducing the learning rate, or by limiting the weights.</li> <li>One or more of the samples used in training are skewed or an outlier. This can lead to the model not being able to reach acceptably tuned weights. This can be solved by excluding edge cases, or by changing the network structure (shape).</li> <li>The model outputs <code>NaN</code> (Not a Number), if the model is not able to output the weights anymore, this usually means that they have exploded (see point 1 above).</li> <li>The model does not seem to reach an acceptable loss, even after hundreds or thousands of epochs. This could be caused because a network structure (shape) was chosen that does not fit well with the data it tries to predict.</li> </ol> <p>If the loss at the end of training is satisfactory (e.g. it has decreased a lot), the weights seem to be tuned well. Now they can be loaded into a model that runs on the TinySpark development kit, predicting gestures.</p> <p>In the next section, the model will be transferred to the TinySpark development kit, and the trained weights will be loaded, in order to test the trained model.</p> <ol> <li> <p>https://en.wikipedia.org/wiki/Loss_function \u21a9</p> </li> <li> <p>https://www.tensorflow.org/ \u21a9</p> </li> </ol>"},{"location":"chapter3/introduction/","title":"Chapter 3 - Training networks","text":"<p>The network shown in the last chapter was still relatively simple. Tuning the weights was still handled manually and by intuition. However, if networks become larger, tuning the optimal weights for each neuron by hand is impossible. Through the use of mathematics, it is possible to calculate the influence of every weight in the network systematically, and determine how to optimally change all weights to get the required output.</p> <p>A walkthrough and explanation of the mathematics will be introduced first, then the calculations will be programmed so they can be performed automatically. Lastly, a model for classifying gestures will be trained using Python and subsequently deployed to the TinySpark development kit. </p>"},{"location":"chapter3/training/","title":"Training networks","text":"<p>In the last chapters, neuron weights were intuitively tuned to produce precise predictions. The mathematical approach to tuning weights will be explained more methodically. </p> <p>To start off, a small recap to the calculation of the output of neurons. First, all inputs get summed together with their weights. Then, an activation function is applied to the result and this gives the final output of a neuron. This is commonly known as the Feedforward of a neural network (as in, feeding inputs forward through a neural network, until an output is generated).</p> <p>In training a neuron, almost the opposite of this process is performed, in the so-called Backpropagation1. To start, a input and the expected output is needed. For this example, a single neuron will be used to show all calculations, and in a later section, the mathematics for a simple network will be explained.</p> <p></p> <p>The neuron above has the following weights, inputs, expected output and new activation function (this is the linear activation function, chosen for it's simplicity).</p> \\[ \\displaylines{ \\text{input1}=0.1\\\\ \\text{input2}=0.8\\\\ \\text{expected 1}=0.4\\\\ \\text{weight1}=0.3\\\\ \\text{weight2}=0.9\\\\ } \\] \\[  f(x) = x \\] <p>The feedforward of this neuron will look as follows.</p> \\[ \\displaylines{ \\sum \\text{inputs}*\\text{weights} = 0.1 * 0.3 + 0.8 * 0.9 = 0.75\\\\ \\text{output}=f(0.75)=0.75\\\\ } \\] <p>As can be seen, the predicted result deviates a considerable amount. Mathematically speaking, this offset is the error of the prediction. It is calculated as follows.</p> \\[ \\displaylines{ \\text{error}=\\text{output}-\\text{expected}\\\\ \\text{error}=0.75-0.4=0.35\\\\ } \\] <p>In order to now calculate the required changes to the weights of the network (called the delta / \\(\\delta\\)), it is nescessary to walk 'backwards' through the network, which in this example only consists of one neuron, and discover the influence of each weight on the final prediction. If the influence is then known, the appropriate changes can be applied to the weights. This technique is called Stochastic Gradient Descent2, but for now, just focus on the application.</p> <p></p> <p>During the feedforward, two steps were performed:</p> <ol> <li>The summation of inputs and weights</li> <li>The activation of the sum</li> </ol> <p>Additionally, one last step is performed at the output of the network, namely the calculation of the error of the prediction.</p> <p></p> <p>To calculate backwards, the derivatives of the mentioned steps need to be determined. For step 2, this requires the derivative of the activation function (in this case the linear function).</p> \\[ \\displaylines{ f(x)=x\\\\ f'(x)=1\\\\ } \\] <p>For step 1, the derivate of the summation is also easy to calculate.</p> \\[ \\displaylines{ \\text{sum}=\\text{input1}*\\text{weight1}+\\text{input2}*\\text{weight2}\\\\ \\text{sum}'_{weight1}=\\text{input1}\\\\ } \\] <p>To calculate the total influence, or delta, all values need to be multiplied together. This is shown for both weights below.</p> \\[ \\displaylines{ \\delta_{weight1}=error*f'(x)*\\text{sum}'_{weight1}=0.35*1*0.1=0.035\\\\ \\delta_{weight2}=error*f'(x)*\\text{sum}'_{weight2}=0.35*1*0.8=0.28\\\\ } \\] <p>Now these deltas can be applied to the weights of the neuron. Note that the delta \\(\\delta\\) is being subtracted from the weight, since there is an overshoot of the initial output.</p> \\[ \\displaylines{ \\text{weight1}_{new}=\\text{weight1}-\\delta_{weight1}=0.3-0.035=0.265\\\\ \\text{weight2}_{new}=\\text{weight2}-\\delta_{weight1}=0.9-0.28=0.62\\\\ } \\] <p>With these new weights, the output of the neuron has come closer to the expected output. To check if this is indeed the case, the error can be re-calculated as well.</p> \\[ \\displaylines{ \\sum \\text{inputs}*\\text{weights}_{new} = 0.1 * 0.265 + 0.8 * 0.62 = 0.5225 \\\\ \\text{output}=f(0.5225)=0.5225\\\\ \\text{error}=0.5225-0.4=0.1225\\\\ } \\] <p>The neuron has now become more accurate at predicting the output. If the above steps are repeated more often, the network will slowly evolve and come closer to an error of \\(0\\). In programming, the repeating training steps are often called epochs, with training for a neural network sometimes needing just 100 epochs to get the desired accuracy, or sometimes requiring more than one million epochs.</p> <p>This training can of course be programmed in Python.</p> <p></p> training_example.py<pre><code># Define the inputs, expected output and weights\ninput1 = 0.1\ninput2 = 0.8\nexpected_output = 0.4\nweight1 = 0.3\nweight2 = 0.9\n# Define the (simple) linear activation function\ndef activation(x):\nreturn x\n# Calculate the output and error of the output\noutput = activation(input1 * weight1 + input2 * weight2)\nerror = output - expected_output\nprint(f\"before training: {output=}, {error=}\")\n# Calculate the deltas for the weights\nd_weight1 = 1 * input1 * error\nd_weight2 = 1 * input2 * error\n# Apply the deltas to the weights\nweight1 = weight1 - d_weight1\nweight2 = weight2 - d_weight2\n# Calculate the output and error of the output again\noutput = activation(input1 * weight1 + input2 * weight2)\nerror = output - expected_output\n# Print the output and new weights\nprint(f\"after training: {output=}, {error=}\")\nprint(f\"new weights: {weight1=},{weight2=}\")\n</code></pre> <p>In training more complicated neural networks, there are a few more nuances to keep in mind. First, in networks with more possible inputs and expected outputs, it is common to run each input through the training calculation and average all deltas for each respective weight before applying them to that weight. That way, no single input / expected output combination will have too much of an influence on the change in weights. </p> \\[\\delta_{weight}=\\frac{\\delta_1+\\delta_2+...+\\delta_n}{n}\\] <p>Next, each neuron in a neural network often has an additional weight applied to its sum called the bias3. This bias, which has an input that is a constant \\(1\\), makes sure that the neuron it is attached to can produce a valid output even when all inputs are zeros. The summation function of a neuron is thus slightly changed as seen below.</p> \\[\\sum (\\text{inputs}*\\text{weights}) + \\text{bias}\\] <p></p> <p>There is often also a learning rate4 applied to the delta before it changes the weight. This learning rate (common rates include 0.1, 0.05 or 0.01) limits the influence of a single change on the weight. The limiting is important to prevent overshooting the expected output(s) and generating weights that are so tiny or large that they have uncontrolled effects on the network. </p> \\[\\text{weight}_{new}=\\text{learning rate}*\\delta_{weight}\\] <p>Lastly, it can be beneficial to split input / expected output combinations in subsets, if there exist a lot of training data. This is called mini-batching5 and  is done to reduce the overall computation- and memory needs to train the network one itteration / epoch. Additionally, it can give the network a faster training cycle as well as quicker approach to the desired accuracy of the model.</p> <p>In the next section, the training of a neural network is programmed in Python, and an example network is trained using gesture data from the TinySpark development kit.</p> <ol> <li> <p>https://en.wikipedia.org/wiki/Backpropagation \u21a9</p> </li> <li> <p>https://en.wikipedia.org/wiki/Stochastic_gradient_descent \u21a9</p> </li> <li> <p>https://machine-learning.paperspace.com/wiki/weights-and-biases \u21a9</p> </li> <li> <p>https://en.wikipedia.org/wiki/Learning_rate \u21a9</p> </li> <li> <p>https://mzuer.github.io/machine_learning/mini_batch \u21a9</p> </li> </ol>"},{"location":"kit/devkit/","title":"TinySpark Development kit","text":"<p>The TinySpark Development kit is a development board specifically tailored to the exploration of TinyML concepts and to test possible applications of TinyML in an easy way. It is based on the popular ESP32-S3 chipset, which is perfect for TinyML applications, because of its fast processor speed and large storage. Additionally it has Bluetooth 5 and WiFi capabilities, making it easy to share sensor data and warn users in case of detections or errors. You can learn more about this chip in its datasheet. The full electronic schematic of the TinyML development kit is available here.</p> <p></p> <p>The TinyML Development kit contains a selection of sensors which enable direct measurements without needing extra hardware, as well as expansion options to adapt the development kit to your specific projects.</p> <p>The Development kit contains the following sensors and I/O:</p> <ul> <li>Inertial motion sensor: this sensor measures both the angular motion using a gyroscope, as well as the acceleration using an accelerometer. The datasheet for the LSM6DS3TR-C sensor can be found here. A template code project for using this sensor can be found here.</li> <li>Microphone: this sensor measures the amplitude and pitch of sound. The datasheet for the ICS-43434 microphone can be found here.</li> <li>Light and Distance sensor: this sensor measures the ambient light level, as well as short-range distance. The datasheet for the APDS-9930 sensor can be found here. A template code project for using this sensor can be found here.</li> <li>Hall effect sensor: this sensor measures the magnetic field around it's axis. The datasheet for the AH-49E sensor can be found here. A template code project for using this sensor can be found here.</li> <li>Environmental sensor: this sensor measures multiple environmental parameters; temperature, relative humidity as well as atmospheric pressure. The datasheet for the BME-280 sensor can be found here. A template code project for using this sensor can be found here.</li> <li>Infrared receiver: this sensor receives infrared signals, for example from a remote control. The datasheet for the IRM-H638T-TR2 receiver can be found here.</li> <li>Buttons: two buttons are accessible to the user (Button 1 and Button 2), which can be configured for a multitude of uses. The last button (Reset) is used if the board needs to be debugged, or if you want to restart your board and your application. A template code project for using the buttons can be found here.</li> <li>Output LED: this Red LED can be used as a simple output, for example to quickly see if your sensor functions correctly. It is connected to the default LED pin, D13. A template code project for using the output LED can be found here.</li> <li>Neopixel LEDs: the Neopixel LEDs can be used to output application specific information, and since they can be programmed to (individually) display every RGB colour imaginable, you can use them to output loads of useful information. The datasheet for the WS2812B programmable RGB LEDs can be found here. A template code project for using the Neopixels can be found here.</li> <li>USB-C connector: this connector is used for the main connection to power and program the board. It can be connected using the included USB-C to USB-A cable.</li> <li>Stemma QT / Qwiic connector: this connector can be used to connect extra sensors to adapt to your specific projects. The connector enables connection to sensors and actuators using the popular Adafruit Stemma QT or Sparkfun Qwiic standards. A template code project for using the Stemma QT / Qwiic connector with an external sensor can be found here.</li> <li>Expansion header: this connector can be used to connect extra peripherals, sensors and actuators to adapt to your specific projects. The connector features power, ground, digital and analog connections, giving you the freedom to connect anything you can think off.</li> </ul> <p>The TinyML development kit also contains some miscellaneous other components such as two green LEDs (for showing power), a Debug header (used to initially program and test the development kit) and some passive components such as resistors and capacitors.</p> Sensor addresses <p>Due to the nature of connections to the sensors on the TinySpark development board, the address on which they are reachable might be different depending on the board you have. Each sensor can be configured for a different address in software, to remedy this issue.</p> <p>The environmental sensor defaults to: <code>address=0x76</code>, however it might also be on <code>address=0x77</code>.</p> <p>The light and distance sensor defaults to <code>address=0x39</code>, this is the only possible address.</p> <p>The inertial motion sensor defaults to <code>address=0x6b</code>, however it might also be on  <code>address=0x6a</code>.</p> Microphone use <p>Due to driver issues in CircuitPython, it is currently not possible to access the microphone data using this programming language. The microphone is still usable in other programming languages such as Arduino and the ESP-IDF. Please see the examples in the given links.</p> <p>There are multiple output pins on the TinyML development board. The specific pinouts are shown in the image below. A template code project for using the expansion pins can be found here. </p> <p></p> <p>The Stemma QT / Qwiic connector uses a different I2C communication bus as the main sensors on the board (on board uses I2C1, Stemma QT / Qwiic uses I2C2). Please see information below to connect sensors to the Stemma QT / Qwiic connector.</p> Secondary I2C bus <p>To connect external sensors to the Stemma QT / Qwiic connector, a new (secondary) I2C bus needs to be initialised. This I2C2-bus is connected to the following pins: <code>SDA2=GPIO17 SCL2=GPIO16</code>.</p> <p>To start the secondary I2C bus, use the template project below.</p> <p></p> secondary_i2c.py<pre><code># import needed libraries\nimport busio\nfrom board import *\n# define the secondary i2c object with the new pins\ni2c2 = busio.I2C(GPIO16, GPIO17)\n# use the i2c2 object below\n</code></pre> <p>In the next section, programming the TinyML Development kit will be explained.</p>"},{"location":"kit/introduction/","title":"Introduction","text":""},{"location":"kit/introduction/#introducing-tinyspark","title":"Introducing TinySpark","text":"<p>These pages give a quick overview of the TinySpark platform and how it works, the TinySpark development kit and all its sensors as well as the programming of neural networks.</p> <p></p> <p>In the next section, the TinySpark platform will be explained. Click on <code>Next</code> to continue the Get Started guide.</p>"},{"location":"kit/platform/","title":"TinySpark platform","text":"<p>This page gives a quick overview of the the TinySpark platform (this webpage), how it functions, how to use it and where to find help if you need it.</p> <p>The TinySpark platform is divided into several chapters:</p> <ul> <li>Getting started (this chapter)</li> <li>Chapter 1: Introduction to neurons</li> <li>Chapter 2: Networks and structures</li> <li>Chapter 3: Training networks</li> <li>Beyond TinySpark</li> </ul> <p>These chapters will introduce various concepts within (Tiny) Machine Learning in an engaging, interactive and project-based way. They can be accessed by clicking on the chapters here, or on the navigation bar at the top of the page (pictured below). Each chapter will explain some theory, let you play around with parts of a neural network and program a mini-project, in which the theory can be put into practise.</p> <p></p> <p>To navigate through chapters, it is possible to use the arrows at the bottom of most pages. Clicking the right-arrow will direct you to the next chapter section. The left-arrow can be used to go back to the previous section of a chapter. Additionally, on ccomputers, there is a side-menu available at the top left of each page, which shows all sections of a chapter.</p> <p></p> <p>The TinySpark platform uses several methods to teach, for example using textual explanation, formulas, code snippets and interactive applications.</p> <p>Source code will be displayed on the page, with the option to open Python code in Google Colaboratory, an online code environment for Python notebooks. Any Python code that can be run on a PC (so no TinyML Development Kit code) will be available for testing and playing around on Colab; just click the link and a new notebook will open. If you want to interact with the code, you need a Google account. </p> <p></p> test_code.py<pre><code># This is some Python code\na = 1\nb = 2\nc = a + b\nprint(c)\n</code></pre> <p>TinyML development board code is hosted on Github, since there is no online platform available for running this code. All TinyML code should be uploaded to the development board to see it in action. Further explanation on running code on the TinyML development kit can be found in the Programming section of this chapter.</p> <p></p> led.py<pre><code># Include all libraries\nimport time\nimport board\nfrom digitalio import DigitalInOut, Direction\n# Initialise LED, declare it an output\nled = DigitalInOut(board.LED)\n# led = DigitalInOut(board.D13)  # Alternatively use the well-known pin 13\nled.direction = Direction.OUTPUT\n# Every second, flash the LED\nwhile True:\nled.value = 0\ntime.sleep(1)\nled.value = 1\ntime.sleep(1)\n</code></pre> <p>Concepts can also be explained using interactive applications. These allow you to manipulate values, play around with network structures and more, seeing the effects in real time.</p> Weight 1 <p>In the next section, the TinyML Development Kit will be introduced. Click on <code>Next</code> to continue the Get Started guide.</p>"},{"location":"kit/programming/","title":"Programming","text":""},{"location":"kit/programming/#programming-the-tinyml-development-kit","title":"Programming the TinyML Development kit","text":"<p>The TinyML Development kit uses the CircuitPython programming language. It is based on the popular Python 3 programming language, and designed to simplify experimentation and learning on microcontrollers and development kits. It requires very little setup on your computer and since the language is based on Python, code is easy to read and understand. Additionally, there are many libraries available, ready to use in your project.</p> <p>The TinyML development kit is programmed using the included USB-C to USB-A cable. If your computer requires another connection (e.g. USB-C to USB-C) you have to provide your own cable for connecting the kit.</p> <p>After connecting the TinyML development kit, two green LEDs on the board should light up, indicating that there is power on the board. At the same time, a new USB-drive named <code>CIRCUITPY</code> should appear on your computer (in File Explorer, Finder or Files). This drive contains code, software libraries and files. To learn more about the <code>CIRCUITPY</code>-drive, take a look at the Adafruit - Circuitpy Drive guide.</p> <p></p> <p>Programming the TinyML development kit is as easy as editing the <code>code.py</code> file that is found on the CIRCUITPY drive. However, to make coding for the kit easier, the Code with Mu IDE will be used. Code with Mu works out of the box with CircuitPython, gives helpful programming prompts and was built with learning in mind.</p> <p>To install the software, head to the Code with Mu - Downloads page and install the correct version of the IDE for your computer.</p> Advanced IDE setup <p>If you are already familiar with programming development kits, or want to work in an IDE which you are familiar with (such as Atom or VS Code), take a look at the Adafruit - Advanced CircuitPython setup.</p> <p>Be aware that all examples are based on the Code with Mu editor, they should function in other editors as well. Setting up the serial connection to the microcontroller can be somewhat more difficult however.</p> <p>After installing, open the Code with Mu editor and (upon first start) select the CircuitPython mode. This ensures that the IDE is set up correctly for use with the TinyML development kit.</p> Reset the Code with Mu editor <p>If you already started Code with Mu and did not select the CircuitPython mode, click the <code>Mode</code> button in the top left of the Code with Mu editor, and select CircuitPython. If you want more information about setting up Code with Mu for CircuitPython, take a look at the Code with Mu - CircuitPython setup.</p> <p></p> <p>Now you can start coding. To begin, try to make the built-in LED (D13) blink.</p> <p></p> led.py<pre><code># Include all libraries\nimport time\nimport board\nfrom digitalio import DigitalInOut, Direction\n# Initialise LED, declare it an output\nled = DigitalInOut(board.LED)\n# led = DigitalInOut(board.D13)  # Alternatively use the well-known pin 13\nled.direction = Direction.OUTPUT\n# Every second, flash the LED\nwhile True:\nled.value = 0\ntime.sleep(1)\nled.value = 1\ntime.sleep(1)\n</code></pre> <p>Copy or write the above code into the Code with Mu editor and click the Save button to save it to the TinyML development board. The red LED should start to blink. Contrary to normal Python, CircuitPython requires the main code file to be named <code>code.py</code> and be located in the root of the <code>CIRCUITPY</code> drive. You can still work with multiple code files, just make sure that the main file is as described.</p> <p></p> <p>If code outputs something (e.g. has a <code>print(\"Hello world!\")</code> statement), the serial console can be used. Through the USB connection, it is possible to read out messages from the <code>print</code> statements using a serial console. To open the Serial console in Code with Mu, click the Serial button at the top of the window. The serial console will now show at the bottom of the window. For more information, take a look at the Adafruit CircuitPython - Serial console page.</p> <p>To check out more simple code examples, take a look at the Adafruit Learning System - Guide to CircuitPython or at the sensor code examples in the previous section.</p> <p>To access inputs and outputs (I/O) of the TinyML development kit, CircuitPython has defined easy references to the default pins. Certain inputs and outputs can be accessed using their semantic definition (e.g. the output connected to the red LED is <code>GPIO13</code>, however it can be referenced  as <code>board.LED</code> in the code). All pin definitions can be found in the info box below. To learn more about the board library and pin usage, visit CircuitPython - Pins and Board.</p> Pin definitions <p>The TinyML development kit has the following pin definitions and names:</p> <p>Sensors</p> <pre><code>board.GPIO14 board.D14 board.IO14 board.LDO2\nboard.GPIO1 board.D1 board.IO1 board.IR board.IR_RECV\nboard.GPIO4 board.D4 board.I2S_CLK board.IO4 board.MIC_CLK\nboard.GPIO5 board.D5 board.I2S_WS board.IO5 board.MIC_WS\nboard.GPIO15 board.D15 board.I2S_DIN board.IO15 board.MIC_DIN\nboard.GPIO2 board.D2 board.HALL board.HALL_EFFECT board.IO2\noard.GPIO21 board.D21 board.IMU board.IMU_INTERRUPT board.IMU_ISR board.IMU_WAKE board.IO21\n</code></pre> <p>Data busses</p> <pre><code>board.GPIO18 board.D18 board.I2C1_SCL board.I2C_SCL board.IO18 board.SCL\nboard.GPIO8 board.D8 board.I2C1_SDA board.I2C_SDA board.IO8 board.SDA\nboard.GPIO16 board.D16 board.I2C2_SCL board.I2C_SCL2 board.IO16 board.SCL2\nboard.GPIO17 board.D17 board.I2C2_SDA board.I2C_SDA2 board.IO17 board.SDA2\nboard.GPIO43 board.D43 board.IO43 board.TX\nboard.GPIO44 board.D44 board.IO44 board.RX\n</code></pre> <p>Expansion header</p> <pre><code>board.GPIO6 board.ANALOG1 board.D6 board.IO6\nboard.GPIO7 board.ANALOG2 board.D7 board.IO7\nboard.GPIO9 board.ANALOG3 board.D9 board.IO9\nboard.GPIO10 board.D10 board.IO10\nboard.GPIO11 board.D11 board.IO11\nboard.GPIO12 board.D12 board.IO12\nboard.GPIO47 board.D47 board.IO47\nboard.GPIO48 board.D48 board.IO48\n</code></pre> <p>Buttons</p> <pre><code>board.GPIO0 board.BUTTON1 board.D0 board.IO0 board.BOOT\nboard.GPIO38 board.BUTTON2 board.D38 board.IO38\n</code></pre> <p>LEDs</p> <pre><code>board.GPIO13 board.D13 board.IO13 board.LED board.STATUS\nboard.GPIO39 board.D39 board.IO39 board.NEOPIXEL\n</code></pre> <p>The original I/O pins of the kit can also be found in the electronic schematic of the TinyML development kit, in the previous section.</p> <p>To see this overview of pin naming on the TinyML development kit, run the following code.</p> <p></p> pin_mapping.py<pre><code># Include the nescessary pins\nimport microcontroller\nimport board\n# Start an empty array for storing the found pins\nboard_pins = []\n# Loop through all pins in the microcontroller directory\nfor pin in dir(microcontroller.pin):\n# Check if a pin has been seen before\nif isinstance(getattr(microcontroller.pin, pin), microcontroller.Pin):\npins = []\n# Add the pin to aliasses if found before\nfor alias in dir(board):\nif getattr(board, alias) is getattr(microcontroller.pin, pin):\npins.append(\"board.{}\".format(alias))\nif len(pins) &gt; 0:\nboard_pins.append(\" \".join(pins))\n# Print all pins\nfor pins in sorted(board_pins):\nprint(pins)\n</code></pre> <p>As mentioned before, it is possible to use ready-made code libraries, for example to easily integrate sensors, outputs or connectivity. Some libraries are built-in to the CircuitPython firmware, others may need to be downloaded and included in the <code>lib</code> folder on the <code>CIRCUITPY</code> drive. The examples on the TinySpark platform only use built-in libraries, as all sensor libraries for the on-board sensors are included in CircuitPython. If you want to learn more about external libraries, visit CircuitPython - Libraries.</p> Included libraries <p>The following libraries are built-in to the default CircuitPython installation on the TinyML development kit</p> <ul> <li><code>analogio</code> (for analog I/O)</li> <li><code>array</code> (for generating array objects)</li> <li><code>bitbangio</code> (for simulating different communication protocols)</li> <li><code>board</code> (for getting pin numbers and descriptions)</li> <li><code>busio</code> (for using default data busses like I2C)</li> <li><code>collections</code> (for special data containers)</li> <li><code>digitalio</code> (for digital I/O)</li> <li><code>math</code> (for basic mathematic operations and constants)</li> <li><code>os</code> (for accessing operating level functions)</li> <li><code>pwmio</code> (for controlling PWM devices)</li> <li><code>random</code> (for generating random numbers)</li> <li><code>register</code> (for attributes on data busses like I2C)</li> <li><code>struct</code> (for defining data structures)</li> <li><code>sys</code> (for accessing system and program functions)</li> <li><code>time</code> (for timing)</li> <li><code>ulab</code> (for Python NumPy like maths and variables)</li> <li><code>usb_cdc</code> (for the USB connection)</li> </ul> <p>The following libraries are used for the sensors and outputs on the TinyML development kit.</p> <ul> <li><code>apds9930</code> (for the on-board Light and Distance sensor)</li> <li><code>bme280</code> (for the on-board Environmental sensor)</li> <li><code>ir_remote</code> (for the on-board IR receiver)</li> <li><code>neopixel</code> (for the on-board Neopixels)</li> <li><code>lsm6ds</code> (for the on-board Inertial motion sensor)</li> </ul> <p>That's it, now you know everything needed to get started with the TinySpark TinyML material.</p> <p>Get started on Chapter 1</p>"}]}